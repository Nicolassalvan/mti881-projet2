{
  "_name_or_path": "bert-base-uncased",
  "architectures": [
    "BertForTokenClassification"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "finetuning_task": "ner",
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "B-T005",
    "1": "B-T007",
    "2": "B-T017",
    "3": "B-T019",
    "4": "B-T020",
    "5": "B-T023",
    "6": "B-T024",
    "7": "B-T025",
    "8": "B-T026",
    "9": "B-T028",
    "10": "B-T031",
    "11": "B-T033",
    "12": "B-T037",
    "13": "B-T038",
    "14": "B-T039",
    "15": "B-T040",
    "16": "B-T041",
    "17": "B-T043",
    "18": "B-T044",
    "19": "B-T045",
    "20": "B-T046",
    "21": "B-T047",
    "22": "B-T048",
    "23": "B-T058",
    "24": "B-T059",
    "25": "B-T062",
    "26": "B-T063",
    "27": "B-T070",
    "28": "B-T074",
    "29": "B-T080",
    "30": "B-T082",
    "31": "B-T098",
    "32": "B-T103",
    "33": "B-T104",
    "34": "B-T109",
    "35": "B-T114",
    "36": "B-T116",
    "37": "B-T121",
    "38": "B-T123",
    "39": "B-T125",
    "40": "B-T129",
    "41": "B-T167",
    "42": "B-T169",
    "43": "B-T170",
    "44": "B-T184",
    "45": "B-T190",
    "46": "B-T191",
    "47": "B-T196",
    "48": "B-T204",
    "49": "I-T005",
    "50": "I-T007",
    "51": "I-T017",
    "52": "I-T019",
    "53": "I-T020",
    "54": "I-T023",
    "55": "I-T024",
    "56": "I-T025",
    "57": "I-T026",
    "58": "I-T028",
    "59": "I-T033",
    "60": "I-T037",
    "61": "I-T038",
    "62": "I-T041",
    "63": "I-T043",
    "64": "I-T044",
    "65": "I-T045",
    "66": "I-T046",
    "67": "I-T047",
    "68": "I-T048",
    "69": "I-T058",
    "70": "I-T059",
    "71": "I-T062",
    "72": "I-T063",
    "73": "I-T070",
    "74": "I-T074",
    "75": "I-T082",
    "76": "I-T098",
    "77": "I-T103",
    "78": "I-T109",
    "79": "I-T114",
    "80": "I-T116",
    "81": "I-T121",
    "82": "I-T125",
    "83": "I-T129",
    "84": "I-T167",
    "85": "I-T170",
    "86": "I-T184",
    "87": "I-T190",
    "88": "I-T191",
    "89": "I-T204",
    "90": "O"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "B-T005": 0,
    "B-T007": 1,
    "B-T017": 2,
    "B-T019": 3,
    "B-T020": 4,
    "B-T023": 5,
    "B-T024": 6,
    "B-T025": 7,
    "B-T026": 8,
    "B-T028": 9,
    "B-T031": 10,
    "B-T033": 11,
    "B-T037": 12,
    "B-T038": 13,
    "B-T039": 14,
    "B-T040": 15,
    "B-T041": 16,
    "B-T043": 17,
    "B-T044": 18,
    "B-T045": 19,
    "B-T046": 20,
    "B-T047": 21,
    "B-T048": 22,
    "B-T058": 23,
    "B-T059": 24,
    "B-T062": 25,
    "B-T063": 26,
    "B-T070": 27,
    "B-T074": 28,
    "B-T080": 29,
    "B-T082": 30,
    "B-T098": 31,
    "B-T103": 32,
    "B-T104": 33,
    "B-T109": 34,
    "B-T114": 35,
    "B-T116": 36,
    "B-T121": 37,
    "B-T123": 38,
    "B-T125": 39,
    "B-T129": 40,
    "B-T167": 41,
    "B-T169": 42,
    "B-T170": 43,
    "B-T184": 44,
    "B-T190": 45,
    "B-T191": 46,
    "B-T196": 47,
    "B-T204": 48,
    "I-T005": 49,
    "I-T007": 50,
    "I-T017": 51,
    "I-T019": 52,
    "I-T020": 53,
    "I-T023": 54,
    "I-T024": 55,
    "I-T025": 56,
    "I-T026": 57,
    "I-T028": 58,
    "I-T033": 59,
    "I-T037": 60,
    "I-T038": 61,
    "I-T041": 62,
    "I-T043": 63,
    "I-T044": 64,
    "I-T045": 65,
    "I-T046": 66,
    "I-T047": 67,
    "I-T048": 68,
    "I-T058": 69,
    "I-T059": 70,
    "I-T062": 71,
    "I-T063": 72,
    "I-T070": 73,
    "I-T074": 74,
    "I-T082": 75,
    "I-T098": 76,
    "I-T103": 77,
    "I-T109": 78,
    "I-T114": 79,
    "I-T116": 80,
    "I-T121": 81,
    "I-T125": 82,
    "I-T129": 83,
    "I-T167": 84,
    "I-T170": 85,
    "I-T184": 86,
    "I-T190": 87,
    "I-T191": 88,
    "I-T204": 89,
    "O": 90
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "torch_dtype": "float32",
  "transformers_version": "4.48.2",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}
